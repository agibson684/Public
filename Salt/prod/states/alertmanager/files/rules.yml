groups:
- name: AptChecks
  rules:
  - alert: AptUpgradePending
    expr:  apt_upgrades_pending >= 5 
    for: 24h
    labels:
      severity: critical
    annotations:
      summary: "Host Updates are pending (instance {{ $labels.instance }})"
      description: "Host has atleast 5 pending update VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
  - alert: AptUpgradePending
    expr:  apt_upgrades_pending >= 1
    for: 24h
    labels:
      severity: warning
    annotations:
      summary: "Host Updates are pending (instance {{ $labels.instance }})"
      description: "Host has atleast 1 pending update VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
- name: HostChecks
  rules:
  - alert: up 
    expr: up == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "HOST IS DOWN! (instance {{ $labels.instance }})"
      description: "The Host is not responding anymore. Please investigate"
  - alert: HostOutOfDiskSpacePool
    expr: (node_filesystem_avail_bytes{mountpoint="/pool"}  * 100) / node_filesystem_size_bytes{mountpoint="/pool"} < 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host out of disk space (instance {{ $labels.instance }})"
      description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"  
  - alert: HostOutOfDiskSpaceLog
    expr: (node_filesystem_avail_bytes{mountpoint="/var/log"}  * 100) / node_filesystem_size_bytes{mountpoint="/var/log"} < 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host out of disk space (instance {{ $labels.instance }})"
      description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"  
  - alert: HostOutOfDiskSpaceHome
    expr: (node_filesystem_avail_bytes{mountpoint="/home"}  * 100) / node_filesystem_size_bytes{mountpoint="/home"} < 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host out of disk space (instance {{ $labels.instance }})"
      description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"  
  - alert: HostOutOfDiskSpaceVar
    expr: (node_filesystem_avail_bytes{mountpoint="/var"}  * 100) / node_filesystem_size_bytes{mountpoint="/var"} < 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host out of disk space (instance {{ $labels.instance }})"
      description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"  
  - alert: HostOutOfDiskSpace
    expr: (node_filesystem_avail_bytes{mountpoint="/"}  * 100) / node_filesystem_size_bytes{mountpoint="/"} < 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host out of disk space (instance {{ $labels.instance }})"
      description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"  
  - alert: PrometheusTargetMissing
    expr: up{instance!="192.168.1.222:9100"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Prometheus target missing (instance {{ $labels.instance }})"
      description: "A Prometheus target has disappeared. An exporter might be crashed.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostOutOfMemory
    expr: node_memory_MemAvailable_bytes{instance!="192.168.1.222:9100"}  / node_memory_MemTotal_bytes{instance!="192.168.1.222:9100"}  * 100 < 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host out of memory (instance {{ $labels.instance }})"
      description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostMemoryUnderMemoryPressure
    expr: rate(node_vmstat_pgmajfault[1m]) > 1000
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host memory under memory pressure (instance {{ $labels.instance }})"
      description: "The node is under heavy memory pressure. High rate of major page faults\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostUnusualNetworkThroughputIn
    expr: sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 200
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host unusual network throughput in (instance {{ $labels.instance }})"
      description: "Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostUnusualNetworkThroughputOut
    expr: sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host unusual network throughput out (instance {{ $labels.instance }})"
      description: "Host network interfaces are probably sending too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostUnusualDiskReadRate
    expr: sum by (instance) (irate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 500
    for: 20m
    labels:
      severity: warning
    annotations:
      summary: "Host unusual disk read rate (instance {{ $labels.instance }})"
      description: "Disk is probably reading too much data (> 500 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostUnusualDiskWriteRate
    expr: sum by (instance) (irate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 500
    for: 20m
    labels:
      severity: warning
    annotations:
      summary: "Host unusual disk write rate (instance {{ $labels.instance }})"
      description: "Disk is probably writing too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostOutOfDiskSpace
    expr: (node_filesystem_avail_bytes{mountpoint="/rootfs"}  * 100) / node_filesystem_size_bytes{mountpoint="/rootfs"} < 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host out of disk space (instance {{ $labels.instance }})"
      description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostDiskWillFillIn4Hours
    expr: predict_linear(node_filesystem_free_bytes{fstype!~"tmpfs"}[1h], 4 * 3600) < 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host disk will fill in 4 hours (instance {{ $labels.instance }})"
      description: "Disk will fill in 4 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostOutOfInodes
    expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint ="/rootfs"} * 100 < 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host out of inodes (instance {{ $labels.instance }})"
      description: "Disk is almost running out of available inodes (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostUnusualDiskReadLatency
    expr: rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 100
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host unusual disk read latency (instance {{ $labels.instance }})"
      description: "Disk latency is growing (read operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostUnusualDiskWriteLatency
    expr: rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 100
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host unusual disk write latency (instance {{ $labels.instance }})"
      description: "Disk latency is growing (write operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostHighCpuLoad
    expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle", instance!~"192.168.1.222:9100|192.168.1.224:9100"}[30m])) * 100) > 80  AND ON() (hour() < 1 OR hour() > 5)
    for: 60m
    labels:
      severity: warning
    annotations:
      summary: "Host high CPU load (instance {{ $labels.instance }})"
      description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  # 1000 context switches is an arbitrary number.
  # Alert threshold depends on nature of application.
  # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58
  - alert: HostContextSwitching
    expr: (rate(node_context_switches_total[5m])) / (count without(cpu, mode) (node_cpu_seconds_total{mode="idle"})) > 35000
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host context switching (instance {{ $labels.instance }})"
      description: "Context switching is growing on node (> 1000 / s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostSwapIsFillingUp
    expr: (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host swap is filling up (instance {{ $labels.instance }})"
      description: "Swap is filling up (>80%)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
## had to edit this one as it was hitting all the mounts! 
  - alert: HostSystemdServiceCrashed
    expr: node_systemd_unit_state{state="failed"} == 1 and node_systemd_unit_state{name!="*.mount"} == 0
   #    expr: node_systemd_unit_state{state="failed"} == 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host SystemD service crashed (instance {{ $labels.instance }})"
      description: "SystemD service crashed\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostPhysicalComponentTooHot
    expr: node_hwmon_temp_celsius > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host physical component too hot (instance {{ $labels.instance }})"
      description: "Physical hardware component too hot\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostNodeOvertemperatureAlarm
    expr: node_hwmon_temp_alarm == 1
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Host node overtemperature alarm (instance {{ $labels.instance }})"
      description: "Physical node temperature alarm triggered\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostRaidArrayGotInactive
    expr: node_md_state{state="inactive"} > 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Host RAID array got inactive (instance {{ $labels.instance }})"
      description: "RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostRaidDiskFailure
    expr: node_md_disks{state="fail"} > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host RAID disk failure (instance {{ $labels.instance }})"
      description: "At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostKernelVersionDeviations
    expr: count(sum(label_replace(node_uname_info, "kernel", "$1", "release", "([0-9]+.[0-9]+.[0-9]+).*")) by (kernel)) > 5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host kernel version deviations (instance {{ $labels.instance }})"
      description: "Different kernel versions are running\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HostOomKillDetected
    expr: increase(node_vmstat_oom_kill[5m]) > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host OOM kill detected (instance {{ $labels.instance }})"
      description: "OOM kill detected\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

# - alert: HostEdacCorrectableErrorsDetected
#    expr: increase(node_edac_correctable_errors_total[5m]) > 0
#    for: 5m
#    labels:
#      severity: info
#    annotations:
#      summary: "Host EDAC Correctable Errors detected (instance {{ $labels.instance }})"
#      description: "{{ $labels.instance }} has had {{ printf "%.0f" $value }} correctable memory errors reported by EDAC in the last 5 minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

#  - alert: HostEdacUncorrectableErrorsDetected
#    expr: node_edac_uncorrectable_errors_total > 0
#    for: 5m
#    labels:
#      severity: warning
#    annotations:
#      summary: "Host EDAC Uncorrectable Errors detected (instance {{ $labels.instance }})"
#      description: "{{ $labels.instance }} has had {{ printf "%.0f" $value }} uncorrectable memory errors reported by EDAC in the last 5 minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

#  - alert: HostNetworkReceiveErrors
#    expr: increase(node_network_receive_errs_total[5m]) > 0
#    for: 5m
#    labels:
#      severity: warning
#    annotations:
#      summary: "Host Network Receive Errors (instance {{ $labels.instance }})"
#      description: "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} receive errors in the last five minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

#  - alert: HostNetworkTransmitErrors
#    expr: increase(node_network_transmit_errs_total[5m]) > 0
#    for: 5m
#    labels:
#      severity: warning
#    annotations:
#      summary: "Host Network Transmit Errors (instance {{ $labels.instance }})"
#      description: "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} transmit errors in the last five minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
- name: PrometheusChecks
  rules:
  - alert: PrometheusConfigurationReloadFailure
    expr: prometheus_config_last_reload_successful != 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Prometheus configuration reload failure (instance {{ $labels.instance }})"
      description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
  - alert: PrometheusTooManyRestarts
    expr: changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 2
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Prometheus too many restarts (instance {{ $labels.instance }})"
      description: "Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
- name: ApacheChecks
  rules:
  - alert: ApacheDown
    expr: apache_up == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Apache down (instance {{ $labels.instance }})"
      description: "Apache down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
  - alert: ApacheCPULoad
    expr: apache_cpuload > 1
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Apache CPU load critical (instance {{ $labels.instance }})"
      description: "Apache CPU load Critical \n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ApacheWorkersLoad
    expr: (sum by (instance) (apache_workers{state="busy"}) / sum by (instance) (apache_scoreboard) ) * 100 > 80
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Apache workers load (instance {{ $labels.instance }})"
      description: "Apache workers in busy state approach the max workers count 80% workers busy on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ApacheWorkersLoad50
    expr: (sum by (instance) (apache_workers{state="busy"}) / sum by (instance) (apache_scoreboard) ) * 100 > 50
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Apache workers load (instance {{ $labels.instance }})"
      description: "Apache workers in busy state approach the max workers count 50% workers busy on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ApacheRestart
    expr: apache_uptime_seconds_total / 60 < 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Apache restart (instance {{ $labels.instance }})"
      description: "Apache has just been restarted, less than one minute ago.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      
  - alert: ApacheScoreboardClosing
    expr: apache_scoreboard{state="closing"} > 5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Apache Scoreboard (instance {{ $labels.instance }})"
      description: "Apache Scoreboard is in warning state .\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ApacheScoreboardDns
    expr: apache_scoreboard{state="dns"} > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Apache Scoreboard (instance {{ $labels.instance }})"
      description: "Apache Scoreboard is in warning state.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ApacheScoreboardGStop
    expr: apache_scoreboard{state="graceful_stop"} > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Apache Scoreboard (instance {{ $labels.instance }})"
      description: "Apache Scoreboard is in warning state.\\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ApacheScoreboardIdle
    expr: apache_scoreboard{state="idle"} < 3
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Apache Scoreboard (instance {{ $labels.instance }})"
      description: "Apache Scoreboard is in warning state.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ApacheScoreboardIdleClean
    expr:  apache_scoreboard{state="idle_cleanup"} > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Apache Scoreboard (instance {{ $labels.instance }})"
      description: "Apache Scoreboard is in warning state.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ApacheScoreboardKeepAlive
    expr: apache_scoreboard{state="keepalive"} > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Apache Scoreboard (instance {{ $labels.instance }})"
      description: "Apache Scoreboard is in warning state.\\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ApacheScoreboardLogging
    expr: apache_scoreboard{state="logging"} > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Apache Scoreboard (instance {{ $labels.instance }})"
      description: "Apache Scoreboard is in warning state.\\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ApacheScoreboardOpenSlot
    expr:  apache_scoreboard{state="open_slot"} < 30
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Apache Scoreboard (instance {{ $labels.instance }})"
      description: "Apache Scoreboard is in warning state.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ApacheScoreboardRead
    expr: apache_scoreboard{state="read"} > 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Apache Scoreboard (instance {{ $labels.instance }})"
      description: "Apache Scoreboard is in warning state.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ApacheScoreboardReply
    expr:  apache_scoreboard{state="reply"} < 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Apache Scoreboard (instance {{ $labels.instance }})"
      description: "Apache Scoreboard is in warning state.\\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ApacheScoreboardStartup
    expr: apache_scoreboard{state="startup"} > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Apache Scoreboard (instance {{ $labels.instance }})"
      description: "Apache Scoreboard is in warning state.\\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: JvmMemoryFillingUp
    expr: jvm_memory_bytes_used / jvm_memory_bytes_max{area="heap"} > 0.8
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "JVM memory filling up (instance {{ $labels.instance }})"
      description: "JVM memory is filling up (> 80%)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
- name: SpeedTestChecks
  rules:
  - alert: SpeedtestSlowInternetDownload
    expr: avg_over_time(speedtest_download[30m]) < 75
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "SpeedTest Slow Internet Download (instance {{ $labels.instance }})"
      description: "Internet download speed is currently {{humanize $value}} Mbps.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: SpeedtestSlowInternetUpload
    expr: avg_over_time(speedtest_upload[30m]) < 20 
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "SpeedTest Slow Internet Upload (instance {{ $labels.instance }})"
      description: "Internet upload speed is currently {{humanize $value}} Mbps.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
- name: SystemdChecks
  rules:
  - alert:  SystemdUnitState_Inactive
    expr: systemd_unit_state{state = "inactive",type = "mount",name=~"media.*"} == 1
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Unmounted CIFS share! (instance {{ $labels.instance }})"
      description: "Unmount CIFS share"
  - alert:  SystemdUnitState_Failed
    expr: systemd_unit_state{state = "failed",type = "mount",name=~"media.*"} == 1
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Unmounted CIFS share! (instance {{ $labels.instance }})"
      description: "Unmount CIFS share"
  - alert:  Plex_State_Failed
    expr: systemd_unit_state{state = "inactive",name="plexmediaserver.service"} == 1  or systemd_unit_state{state = "failed",name="plexmediaserver.service"} == 1 
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Plexmediaserver is failed! (instance {{ $labels.instance }})"
      description: "Plexmediaserver is failed!"
  - alert:  Saltmaster_State_Failed
    expr: systemd_unit_state{state = "inactive",name="salt-master.service"} == 1  or systemd_unit_state{state = "failed",name="salt-master.service"} == 1 
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Salt-Master is failed! (instance {{ $labels.instance }})"
      description: "Salt-Master is failed!"
  - alert:  Saltapi_State_Failed
    expr: systemd_unit_state{state = "inactive",name="salt-api.service"} == 1  or systemd_unit_state{state = "failed",name="salt-api.service"} == 1 
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Salt-Api is failed! (instance {{ $labels.instance }})"
      description: "Salt-Api is failed!"
  - alert:  Saltminion_State_Failed
    expr: systemd_unit_state{state = "inactive",name="salt-minion.service"} == 1  or systemd_unit_state{state = "failed",name="salt-minion.service"} == 1 
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Salt-minion is failed! (instance {{ $labels.instance }})"
      description: "Salt-minion is failed!"

#- name: BindChecks
#  rules:
#  - alert: Bind_Query_Stats_Last_Scrape_Error  
#    expr:  bind_query_stats_last_scrape_error > 0
#    for: 5m
#    labels:
#      severity: critical
#    annotations:
#      summary: "Bind Query Errors! (instance {{ $labels.instance }})"
#      description: "Bind Query Errors!"
#  - alert: Bind_Query_Stats_Scrape_Errors_Total 
#    expr:   bind_query_stats_scrape_errors_total > 0
#    for: 5m
#    labels:
#      severity: critical
#    annotations:
#      summary: "Bind Query Stat Scrape Error Total is (instance {{ $labels.instance }})"
#      description: "Bind Query Stat Scrape Error Total is greater than ZERO" 
#  - alert: Bind_Query_Errors_Total 
#    expr:  rate(bind_query_errors_total[24h]) > 0
#    for: 5m
#    labels:
#      severity: critical
#    annotations:
#      summary: "Bind Query  Error Total is (instance {{ $labels.instance }})"
#      description: "Bind Query Error Total is greater than ZERO" 
#  - alert: Bind_Down
#    expr:   bind_up != 1
#    for: 5m
#    labels:
#      severity: critical
#    annotations:
#      summary: "Bind is Down (instance {{ $labels.instance }})"
#      description: "Bind is Down" 
#  - alert: Bind_Resolver_Response_Errors_Total 
#    expr:   bind_resolver_response_errors_total > 100000
#    for: 5m
#    labels:
#      severity: critical
#    annotations:
#      summary: "Bind Resolver Reponse Error Total is (instance {{ $labels.instance }})"
#      description: "Bind Resolver Response Error Total is greater than 100" 
#     

- name: MysqlChecks
  rules:
  - alert: Mysql_Down
    expr: mysql_up != 1
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Mysql Service is Down! (instance {{ $labels.instance }})"
      description: "Mysql Service is Down!"

- name: SmartMonChecks
  rules:
  - alert: Smart_NotHealthy_Calisto
    expr:  smartmon_device_smart_healthy{instance="192.168.1.220:9100"} ==0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Calisto Devices not Healthy (instance {{ $labels.instance }})"
      description: "Calisto Devices not Healthy"
  - alert: Smart_NotHealthy_Gabrielle
    expr: smartmon_device_smart_healthy{instance="192.168.1.201:9100"} ==0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Gabrielle Devices not Healthy (instance {{ $labels.instance }})"
      description: "Gabrielle Devices not Healthy"
- name: PingChecks
  rules:
  - alert: PingChecks
    expr:  ping_loss_percent > 1 
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "ping Loss (instance {{ $labels.instance }})"
      description: "ping loss"
#  - alert: ConnectionChecksAthena
#    expr: connection_status_up{host="athena.thrace-lan.info"} != 1 
#    for: 30s
#    labels:
#      severity: critical
#    annotations:
#      summary: "Athena is down! (instance {{ $labels.instance }})"
#      description: "Athena is down!"
  - alert: ConnectionChecksZeus
    expr: connection_status_up{host="zeus.thrace-lan.info"} != 1 
    for: 30s 
    labels:
      severity: critical
    annotations:
      summary: "Zeus is down! (instance {{ $labels.instance }})"
      description: "Zeus is down!"
  - alert: ConnectionChecksTransmission
    expr: connection_status_up{name="transmission-http"} != 1 
    for: 30s
    labels:
      severity: critical
    annotations:
      summary: "Bittorrent is down! (instance {{ $labels.instance }})"
      description: "Bittorrent is down!"
  - alert: ConnectionChecksPlex
    expr: connection_status_up{name="plex-tv"} != 1 
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "plex is down! (instance {{ $labels.instance }})"
      description: "plex is down!"
- name: digitalocean.rules
  rules:
  - record: digitalocean_droplets_price_monthly
    expr: sum(digitalocean_droplet_price_monthly)
  - record: digitalocean_snapshots_price_monthly
    expr: sum(digitalocean_snapshot_size_bytes) / 1024 / 1024 / 1024 / 20
  - record: digitalocean_volumes_price_monthly
    expr: sum(digitalocean_volume_size_bytes) / 1024 / 1024 / 1024 / 10
  - record: digitalocean_price_monthly
    expr: digitalocean_droplets_price_monthly + digitalocean_volumes_price_monthly + digitalocean_snapshots_price_monthly
  - alert: droplet_down
    expr: digitalocean_droplet_up == 0
    for: 5m
    annotations:
      description: Droplet {{ $labels.name }} in region {{ $labels.region }} is down.
      summary: Droplet is down.
  - alert: high_monthly_price
    expr: digitalocean_price_monthly > 100
    for: 6h
    annotations:
      description: Overall we're spending too much money. Please try to minimize cost.
      summary: Paying too much at DigitalOcean.
  - alert: floating_ip_unused
    expr: digitalocean_floating_ipv4_active == 0
    for: 1h
    annotations:
      description: Paying ~5$/month for {{ $labels.ipv4 }}, an unused Floating IP.
      summary: Paying for an unsed FloatingIP.
  - alert: too_many_packer_snapshots
    expr: count(digitalocean_snapshot_size_bytes{name=~"packer-.*",type="droplet"})
      > 10
    for: 6h
    annotations:
      description: Please delete old packer snapshots. We don't need {{$value}}.
      summary: More than 10 Packer Snapshots.
  - alert: too_few_ssh_keys
    expr: count(digitalocean_key) < 1
    for: 1h
    annotations:
      description: We can't find SSH keys, please add at least one.
      summary: No SSH Keys.
